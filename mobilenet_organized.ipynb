{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3G7RfLWnLHh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본 라이브러리\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# 이미지 처리\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# 머신러닝 관련\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve\n",
        ")\n",
        "\n",
        "# TensorFlow/Keras 관련\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import (\n",
        "    MobileNet,\n",
        "    MobileNetV2,\n",
        "    MobileNetV3Small,\n",
        "    MobileNetV3Large\n",
        ")\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input  # Input 추가\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 기타 유틸리티\n",
        "from tqdm import tqdm\n",
        "from itertools import cycle"
      ],
      "metadata": {
        "id": "YB1-6e31nNPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로드 및 확인\n",
        "def get_data_paths(base_path):\n",
        "    classes = ['CN7', 'G2', 'BLDC-400-f', 'BLDC-400-b']\n",
        "    data_paths = {}\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_path = os.path.join(base_path, class_name)\n",
        "        if os.path.exists(class_path):\n",
        "            # 해당 클래스 디렉토리의 모든 이미지 파일 경로 수집\n",
        "            image_paths = []\n",
        "            for file_name in os.listdir(class_path):\n",
        "                image_path = os.path.join(class_path, file_name)\n",
        "                if os.path.isfile(image_path):  # 파일인 경우만 추가\n",
        "                    image_paths.append(image_path)\n",
        "            data_paths[class_name] = image_paths\n",
        "\n",
        "    return data_paths\n",
        "\n",
        "base_path = '/content/drive/MyDrive/augmented_data'\n",
        "paths = get_data_paths(base_path)\n",
        "\n",
        "# 각 클래스별 이미지 개수 출력\n",
        "for class_name, image_paths in paths.items():\n",
        "    print(f\"{class_name}: {len(image_paths)} images\")\n",
        "\n",
        "# 클래스 별 예시 이미지 1개씩 출력\n",
        "def display_one_per_class(paths):\n",
        "  num_classes = len(paths)\n",
        "  plt.figure(figsize=(15, 3))\n",
        "\n",
        "  for idx, (class_name, image_paths) in enumerate(paths.items(), 1):\n",
        "       # 첫 번째 이미지만 선택\n",
        "       img_path = image_paths[0]\n",
        "\n",
        "       plt.subplot(1, num_classes, idx)\n",
        "       img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "       plt.imshow(img, cmap='gray')\n",
        "       plt.title(f'{class_name}')\n",
        "       plt.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# 각 클래스별 1개 이미지 표시\n",
        "display_one_per_class(paths)"
      ],
      "metadata": {
        "id": "PObGBKXCnNTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#V1,V2"
      ],
      "metadata": {
        "id": "HLNDh_97nNVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curves(y_test, y_pred_proba, class_names):\n",
        "    \"\"\"다중 클래스 ROC 커브 플로팅\"\"\"\n",
        "    n_classes = len(class_names)\n",
        "\n",
        "    # 원-핫 인코딩으로 변환\n",
        "    y_test_bin = tf.keras.utils.to_categorical(y_test, n_classes)\n",
        "\n",
        "    # 각 클래스별 ROC 커브 계산\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Plot all ROC curves\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
        "\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                label=f'{class_names[i]} (AUC = {roc_auc[i]:0.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curves')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def calculate_map(y_test, y_pred_proba, class_names):\n",
        "    \"\"\"클래스별 AP와 mAP 계산\"\"\"\n",
        "    n_classes = len(class_names)\n",
        "    y_test_bin = tf.keras.utils.to_categorical(y_test, n_classes)\n",
        "\n",
        "    # 클래스별 AP 계산\n",
        "    ap_scores = {}\n",
        "    for i in range(n_classes):\n",
        "        ap = average_precision_score(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "        ap_scores[class_names[i]] = ap\n",
        "\n",
        "    # mAP 계산\n",
        "    map_score = np.mean(list(ap_scores.values()))\n",
        "\n",
        "    print(\"\\n=== Average Precision Scores ===\")\n",
        "    for class_name, ap in ap_scores.items():\n",
        "        print(f\"{class_name}: {ap:.4f}\")\n",
        "    print(f\"\\nMean Average Precision (mAP): {map_score:.4f}\")\n",
        "\n",
        "    return ap_scores, map_score\n",
        "\n",
        "def load_data(base_path, num_samples=40):\n",
        "    transforms = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    class_names = ['CN7', 'G2', 'BLDC-400-f', 'BLDC-400-b']\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    print(\"\\n데이터 로딩 시작:\")\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(base_path, class_name)\n",
        "        if os.path.exists(class_path):\n",
        "            files = []\n",
        "            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "                files.extend([f for f in os.listdir(class_path)\n",
        "                            if f.lower().endswith(ext)])\n",
        "            files = sorted(files)[:num_samples]\n",
        "\n",
        "            print(f\"{class_name} 클래스 로딩 중: {len(files)}개 이미지\")\n",
        "\n",
        "            for file in tqdm(files):\n",
        "                image_path = os.path.join(class_path, file)\n",
        "                try:\n",
        "                    image = cv2.imread(image_path)\n",
        "                    if image is None:\n",
        "                        print(f\"이미지를 읽을 수 없음: {image_path}\")\n",
        "                        continue\n",
        "\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    transformed = transforms(image=image)['image']\n",
        "                    transformed = transformed.numpy().transpose(1, 2, 0)\n",
        "\n",
        "                    images.append(transformed)\n",
        "                    labels.append(class_names.index(class_name))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"이미지 처리 중 에러 발생 {image_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def load_test_data(test_path):\n",
        "    transforms = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    class_names = ['CN7', 'G2', 'BLDC-400-f', 'BLDC-400-b']\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    print(\"\\n테스트 데이터 로딩:\")\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(test_path, class_name)\n",
        "        if os.path.exists(class_path):\n",
        "            files = []\n",
        "            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "                files.extend([f for f in os.listdir(class_path)\n",
        "                            if f.lower().endswith(ext)])\n",
        "            files = sorted(files)\n",
        "\n",
        "            print(f\"{class_name} 클래스 발견: {len(files)}개 이미지\")\n",
        "\n",
        "            for file in tqdm(files):\n",
        "                image_path = os.path.join(class_path, file)\n",
        "                try:\n",
        "                    image = cv2.imread(image_path)\n",
        "                    if image is None:\n",
        "                        print(f\"이미지를 읽을 수 없음: {image_path}\")\n",
        "                        continue\n",
        "\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    transformed = transforms(image=image)['image']\n",
        "                    transformed = transformed.numpy().transpose(1, 2, 0)\n",
        "\n",
        "                    images.append(transformed)\n",
        "                    labels.append(class_names.index(class_name))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"이미지 처리 중 에러 발생 {image_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred, y_pred_proba, class_names):\n",
        "    \"\"\"모델의 성능을 종합적으로 평가하는 함수\"\"\"\n",
        "    metrics = {\n",
        "        'accuracy': np.mean(y_true == y_pred),\n",
        "        'macro_precision': precision_score(y_true, y_pred, average='macro'),\n",
        "        'macro_recall': recall_score(y_true, y_pred, average='macro'),\n",
        "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
        "        'weighted_precision': precision_score(y_true, y_pred, average='weighted'),\n",
        "        'weighted_recall': recall_score(y_true, y_pred, average='weighted'),\n",
        "        'weighted_f1': f1_score(y_true, y_pred, average='weighted'),\n",
        "    }\n",
        "\n",
        "    class_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro')\n",
        "        metrics['roc_auc'] = roc_auc\n",
        "    except:\n",
        "        metrics['roc_auc'] = None\n",
        "\n",
        "    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    return {\n",
        "        'basic_metrics': metrics,\n",
        "        'class_report': class_report,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'class_accuracy': dict(zip(class_names, class_accuracy))\n",
        "    }\n",
        "\n",
        "class PerformanceThresholdCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, accuracy_threshold=0.90, loss_threshold=0.3):\n",
        "        super().__init__()\n",
        "        self.best_weights = None\n",
        "        self.best_accuracy = 0\n",
        "        self.accuracy_threshold = accuracy_threshold\n",
        "        self.loss_threshold = loss_threshold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current_accuracy = logs.get('val_accuracy', 0)\n",
        "        current_loss = logs.get('val_loss', float('inf'))\n",
        "\n",
        "        if (current_accuracy >= self.accuracy_threshold and\n",
        "            current_loss <= self.loss_threshold and\n",
        "            current_accuracy > self.best_accuracy):\n",
        "            print(f\"\\n성능 임계값 달성! (accuracy: {current_accuracy:.4f}, loss: {current_loss:.4f})\")\n",
        "            self.best_accuracy = current_accuracy\n",
        "            self.best_weights = self.model.get_weights()\n",
        "            print(f\"새로운 best weights 저장됨 (정확도: {current_accuracy:.4f})\")\n",
        "\n",
        "        if current_accuracy >= 0.99 and current_loss < 0.05:\n",
        "            print(f\"\\n최종 목표 성능 도달! (accuracy: {current_accuracy:.4f}, loss: {current_loss:.4f})\")\n",
        "            if self.best_weights is not None:\n",
        "                self.model.set_weights(self.best_weights)\n",
        "                print(\"최고 성능 모델의 가중치로 복원됨\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class MobileNetExperiment:\n",
        "    def __init__(self, version, num_classes=4, input_shape=(224, 224, 3)):\n",
        "        self.version = version\n",
        "        self.num_classes = num_classes\n",
        "        self.input_shape = input_shape\n",
        "        self.model = self._create_model()\n",
        "        self.history = None\n",
        "        self.training_time = None\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        \"\"\"모델을 저장하는 함수\"\"\"\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        model_path = os.path.join(save_path, f'mobilenet_{self.version}_model.h5')\n",
        "        self.model.save(model_path)\n",
        "\n",
        "        metadata = {\n",
        "            'version': self.version,\n",
        "            'best_val_accuracy': self.best_val_accuracy,\n",
        "            'training_time': self.training_time,\n",
        "            'history': self.history.history if self.history else None\n",
        "        }\n",
        "\n",
        "        metadata_path = os.path.join(save_path, f'mobilenet_{self.version}_metadata.json')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f)\n",
        "\n",
        "        print(f\"모델이 저장되었습니다: {model_path}\")\n",
        "        print(f\"메타데이터가 저장되었습니다: {metadata_path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, load_path, version):\n",
        "        \"\"\"저장된 모델을 불러오는 함수\"\"\"\n",
        "        instance = cls(version)\n",
        "\n",
        "        model_path = os.path.join(load_path, f'mobilenet_{version}_model.h5')\n",
        "        metadata_path = os.path.join(load_path, f'mobilenet_{version}_metadata.json')\n",
        "\n",
        "        if os.path.exists(model_path):\n",
        "            instance.model = tf.keras.models.load_model(model_path)\n",
        "            print(f\"모델을 불러왔습니다: {model_path}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"모델 파일을 찾을 수 없습니다: {model_path}\")\n",
        "\n",
        "        if os.path.exists(metadata_path):\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "\n",
        "            instance.best_val_accuracy = metadata['best_val_accuracy']\n",
        "            instance.training_time = metadata['training_time']\n",
        "            if metadata['history']:\n",
        "                instance.history = type('History', (), {'history': metadata['history']})\n",
        "\n",
        "            print(f\"메타데이터를 불러왔습니다: {metadata_path}\")\n",
        "\n",
        "        return instance\n",
        "\n",
        "    def _create_model(self):\n",
        "        if self.version == 'v1':\n",
        "            base_model = MobileNet(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape\n",
        "            )\n",
        "        elif self.version == 'v2':\n",
        "            base_model = MobileNetV2(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape\n",
        "            )\n",
        "        elif self.version == 'v3-small':\n",
        "            base_model = MobileNetV3Small(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape\n",
        "            )\n",
        "        elif self.version == 'v3-large':\n",
        "            base_model = MobileNetV3Large(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape\n",
        "            )\n",
        "\n",
        "        base_model.trainable = False\n",
        "\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        predictions = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = Model(inputs=base_model.input, outputs=predictions)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=10):\n",
        "        print(f\"\\n=== MobileNet {self.version} 학습 시작 ===\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            min_delta=0.001,\n",
        "            patience=6,\n",
        "            mode='max',\n",
        "            verbose=1,\n",
        "            restore_best_weights=False\n",
        "        )\n",
        "\n",
        "        performance_threshold = PerformanceThresholdCallback(\n",
        "            accuracy_threshold=1,  # 100% 정확도\n",
        "            loss_threshold=0.05    # 0.05 이하 손실\n",
        "        )\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            callbacks=[early_stop, performance_threshold],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        if performance_threshold.best_weights is not None:\n",
        "            print(\"\\n임계값을 넘는 best weights가 발견되어 복원합니다.\")\n",
        "            self.model.set_weights(performance_threshold.best_weights)\n",
        "            self.best_val_accuracy = performance_threshold.best_accuracy\n",
        "        else:\n",
        "            print(\"\\n임계값을 넘는 모델이 없어 현재 weights를 유지합니다.\")\n",
        "            self.best_val_accuracy = max(self.history.history['val_accuracy'])\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        print(f\"학습 시간: {self.training_time:.2f}초\")\n",
        "\n",
        "    def evaluate(self, X_test, y_test, class_names):\n",
        "        inference_start_time = time.time()\n",
        "\n",
        "        # 예측 수행\n",
        "        y_pred_proba = self.model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "        inference_time = time.time() - inference_start_time\n",
        "\n",
        "        # 종합적인 성능 평가\n",
        "        evaluation_results = evaluate_metrics(y_test, y_pred, y_pred_proba, class_names)\n",
        "\n",
        "        # ROC 커브 그리기\n",
        "        plot_roc_curves(y_test, y_pred_proba, class_names)\n",
        "\n",
        "        # MAP 계산\n",
        "        ap_scores, map_score = calculate_map(y_test, y_pred_proba, class_names)\n",
        "\n",
        "        # 결과 출력\n",
        "        print(\"\\n=== 모델 성능 평가 결과 ===\")\n",
        "        print(f\"\\n1. 기본 메트릭:\")\n",
        "        for metric, value in evaluation_results['basic_metrics'].items():\n",
        "            if value is not None:\n",
        "                print(f\"- {metric}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\n2. 클래스별 성능:\")\n",
        "        for class_name in class_names:\n",
        "            metrics = evaluation_results['class_report'][class_name]\n",
        "            print(f\"\\n{class_name}:\")\n",
        "            print(f\"- Precision: {metrics['precision']:.4f}\")\n",
        "            print(f\"- Recall: {metrics['recall']:.4f}\")\n",
        "            print(f\"- F1-score: {metrics['f1-score']:.4f}\")\n",
        "            print(f\"- Support: {metrics['support']}\")\n",
        "\n",
        "        print(f\"\\n3. 처리 시간:\")\n",
        "        print(f\"- 전체 추론 시간: {inference_time:.2f}초\")\n",
        "        print(f\"- 이미지당 평균 추론 시간: {(inference_time/len(X_test))*1000:.2f}ms\")\n",
        "\n",
        "        self._plot_training_history()\n",
        "        self._plot_confusion_matrix(evaluation_results['confusion_matrix'], class_names)\n",
        "\n",
        "        return {\n",
        "            'metrics': evaluation_results['basic_metrics'],\n",
        "            'class_report': evaluation_results['class_report'],\n",
        "            'confusion_matrix': evaluation_results['confusion_matrix'],\n",
        "            'class_accuracy': evaluation_results['class_accuracy'],\n",
        "            'training_time': self.training_time,\n",
        "            'inference_time': inference_time,\n",
        "            'inference_time_per_image': inference_time/len(X_test),\n",
        "            'best_val_accuracy': self.best_val_accuracy,\n",
        "            'best_val_loss': min(self.history.history['val_loss']),\n",
        "            'ap_scores': ap_scores,\n",
        "            'map_score': map_score\n",
        "        }\n",
        "\n",
        "    def _plot_training_history(self):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history.history['accuracy'], label='train')\n",
        "        plt.plot(self.history.history['val_accuracy'], label='validation')\n",
        "        plt.title(f'MobileNet {self.version} Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.history.history['loss'], label='train')\n",
        "        plt.plot(self.history.history['val_loss'], label='validation')\n",
        "        plt.title(f'MobileNet {self.version} Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm, class_names):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names\n",
        "        )\n",
        "        plt.title(f'MobileNet {self.version} Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "def run_single_experiment(version, X_train, y_train, X_val, y_val, X_test, y_test, class_names):\n",
        "    experiment = MobileNetExperiment(version)\n",
        "    experiment.train(X_train, y_train, X_val, y_val)\n",
        "    results = experiment.evaluate(X_test, y_test, class_names)\n",
        "\n",
        "    print(f\"\\n=== MobileNet {version} 실험 결과 ===\")\n",
        "    print(f\"최고 검증 정확도: {results['best_val_accuracy']:.4f}\")\n",
        "    print(f\"최고 검증 loss: {results['best_val_loss']:.4f}\")\n",
        "    print(f\"학습 소요 시간: {results['training_time']:.2f}초\")\n",
        "    print(f\"전체 추론 시간: {results['inference_time']:.2f}초\")\n",
        "    print(f\"이미지당 평균 추론 시간: {results['inference_time_per_image']*1000:.2f}ms\")\n",
        "    print(f\"테스트 정확도: {results['metrics']['accuracy']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. 경로 설정\n",
        "    base_path = '/content/drive/MyDrive/augmented_data'\n",
        "    test_path = '/content/drive/MyDrive/test_data2'\n",
        "    model_save_path = '/content/drive/MyDrive/saved_models'  # 모델 저장 경로\n",
        "\n",
        "    # 2. 데이터 로드\n",
        "    print(\"=== 데이터 로드 시작 ===\")\n",
        "    train_images, train_labels = load_data(base_path, num_samples=102)\n",
        "    test_images, test_labels = load_test_data(test_path)\n",
        "\n",
        "    # 3. 학습/검증 데이터 분할\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        train_images,\n",
        "        train_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=train_labels\n",
        "    )\n",
        "\n",
        "    # 4. 모델 학습\n",
        "    version = 'v2'\n",
        "    experiment = MobileNetExperiment(version)\n",
        "    experiment.train(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # 5. 모델 저장\n",
        "    experiment.save_model(model_save_path)\n",
        "\n",
        "    #6. 모델 불러오기 및 평가\n",
        "    loaded_experiment = MobileNetExperiment.load_model(model_save_path, version)\n",
        "    class_names = ['CN7', 'G2', 'BLDC-400-f', 'BLDC-400-b']\n",
        "    results = loaded_experiment.evaluate(test_images, test_labels, class_names)"
      ],
      "metadata": {
        "id": "dm9p9RmTny2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#V3(small,large) V1,V2 랑 레이어 달라서 코드 float 함수 바꿔서 넣어야함"
      ],
      "metadata": {
        "id": "lhS2bFZ8oSmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curves(y_test, y_pred_proba, class_names):\n",
        "    \"\"\"다중 클래스 ROC 커브 플로팅\"\"\"\n",
        "    n_classes = len(class_names)\n",
        "    y_test_bin = tf.keras.utils.to_categorical(y_test, n_classes)\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
        "\n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "                label=f'{class_names[i]} (AUC = {roc_auc[i]:0.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curves')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def calculate_map(y_test, y_pred_proba, class_names):\n",
        "    \"\"\"클래스별 AP와 mAP 계산\"\"\"\n",
        "    n_classes = len(class_names)\n",
        "    y_test_bin = tf.keras.utils.to_categorical(y_test, n_classes)\n",
        "\n",
        "    ap_scores = {}\n",
        "    for i in range(n_classes):\n",
        "        ap = average_precision_score(y_test_bin[:, i], y_pred_proba[:, i])\n",
        "        ap_scores[class_names[i]] = ap\n",
        "\n",
        "    map_score = np.mean(list(ap_scores.values()))\n",
        "\n",
        "    print(\"\\n=== Average Precision Scores ===\")\n",
        "    for class_name, ap in ap_scores.items():\n",
        "        print(f\"{class_name}: {ap:.4f}\")\n",
        "    print(f\"\\nMean Average Precision (mAP): {map_score:.4f}\")\n",
        "\n",
        "    return ap_scores, map_score\n",
        "\n",
        "def load_data(base_path, num_samples=40):\n",
        "    transforms = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    class_names = ['CN7', 'G2', 'BLDC-400-f', 'BLDC-400-b']\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    print(\"\\n데이터 로딩 시작:\")\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(base_path, class_name)\n",
        "        if os.path.exists(class_path):\n",
        "            files = []\n",
        "            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "                files.extend([f for f in os.listdir(class_path)\n",
        "                            if f.lower().endswith(ext)])\n",
        "            files = sorted(files)[:num_samples]\n",
        "\n",
        "            print(f\"{class_name} 클래스 로딩 중: {len(files)}개 이미지\")\n",
        "\n",
        "            for file in tqdm(files):\n",
        "                image_path = os.path.join(class_path, file)\n",
        "                try:\n",
        "                    image = cv2.imread(image_path)\n",
        "                    if image is None:\n",
        "                        print(f\"이미지를 읽을 수 없음: {image_path}\")\n",
        "                        continue\n",
        "\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    transformed = transforms(image=image)['image']\n",
        "                    transformed = transformed.numpy().transpose(1, 2, 0)\n",
        "\n",
        "                    images.append(transformed)\n",
        "                    labels.append(class_names.index(class_name))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"이미지 처리 중 에러 발생 {image_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def load_test_data(test_path):\n",
        "    transforms = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    class_names = ['CN7', 'G2', 'BLDC-400-f', 'BLDC-400-b']\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    print(\"\\n테스트 데이터 로딩:\")\n",
        "    for class_name in class_names:\n",
        "        class_path = os.path.join(test_path, class_name)\n",
        "        if os.path.exists(class_path):\n",
        "            files = []\n",
        "            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "                files.extend([f for f in os.listdir(class_path)\n",
        "                            if f.lower().endswith(ext)])\n",
        "            files = sorted(files)\n",
        "\n",
        "            print(f\"{class_name} 클래스 발견: {len(files)}개 이미지\")\n",
        "\n",
        "            for file in tqdm(files):\n",
        "                image_path = os.path.join(class_path, file)\n",
        "                try:\n",
        "                    image = cv2.imread(image_path)\n",
        "                    if image is None:\n",
        "                        print(f\"이미지를 읽을 수 없음: {image_path}\")\n",
        "                        continue\n",
        "\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                    transformed = transforms(image=image)['image']\n",
        "                    transformed = transformed.numpy().transpose(1, 2, 0)\n",
        "\n",
        "                    images.append(transformed)\n",
        "                    labels.append(class_names.index(class_name))\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"이미지 처리 중 에러 발생 {image_path}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def evaluate_metrics(y_true, y_pred, y_pred_proba, class_names):\n",
        "    \"\"\"모델의 성능을 종합적으로 평가하는 함수\"\"\"\n",
        "    metrics = {\n",
        "        'accuracy': np.mean(y_true == y_pred),\n",
        "        'macro_precision': precision_score(y_true, y_pred, average='macro'),\n",
        "        'macro_recall': recall_score(y_true, y_pred, average='macro'),\n",
        "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
        "        'weighted_precision': precision_score(y_true, y_pred, average='weighted'),\n",
        "        'weighted_recall': recall_score(y_true, y_pred, average='weighted'),\n",
        "        'weighted_f1': f1_score(y_true, y_pred, average='weighted'),\n",
        "    }\n",
        "\n",
        "    class_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    try:\n",
        "        roc_auc = roc_auc_score(y_true, y_pred_proba, multi_class='ovr', average='macro')\n",
        "        metrics['roc_auc'] = roc_auc\n",
        "    except:\n",
        "        metrics['roc_auc'] = None\n",
        "\n",
        "    class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    return {\n",
        "        'basic_metrics': metrics,\n",
        "        'class_report': class_report,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'class_accuracy': dict(zip(class_names, class_accuracy))\n",
        "    }\n",
        "\n",
        "class PerformanceThresholdCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, accuracy_threshold=0.90, loss_threshold=0.3):\n",
        "        super().__init__()\n",
        "        self.best_weights = None\n",
        "        self.best_accuracy = 0\n",
        "        self.accuracy_threshold = accuracy_threshold\n",
        "        self.loss_threshold = loss_threshold\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current_accuracy = logs.get('val_accuracy', 0)\n",
        "        current_loss = logs.get('val_loss', float('inf'))\n",
        "\n",
        "        if (current_accuracy >= self.accuracy_threshold and\n",
        "            current_loss <= self.loss_threshold and\n",
        "            current_accuracy > self.best_accuracy):\n",
        "            print(f\"\\n성능 임계값 달성! (accuracy: {current_accuracy:.4f}, loss: {current_loss:.4f})\")\n",
        "            self.best_accuracy = current_accuracy\n",
        "            self.best_weights = self.model.get_weights()\n",
        "            print(f\"새로운 best weights 저장됨 (정확도: {current_accuracy:.4f})\")\n",
        "\n",
        "        if current_accuracy >= 0.99 and current_loss < 0.05:\n",
        "            print(f\"\\n최종 목표 성능 도달! (accuracy: {current_accuracy:.4f}, loss: {current_loss:.4f})\")\n",
        "            if self.best_weights is not None:\n",
        "                self.model.set_weights(self.best_weights)\n",
        "                print(\"최고 성능 모델의 가중치로 복원됨\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class MobileNetExperiment:\n",
        "    def __init__(self, version, num_classes=4, input_shape=(224, 224, 3)):\n",
        "        self.version = version\n",
        "        self.num_classes = num_classes\n",
        "        self.input_shape = input_shape\n",
        "        self.model = self._create_model()\n",
        "        self.history = None\n",
        "        self.training_time = None\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def _create_model(self):\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "\n",
        "        if self.version == 'v1':\n",
        "            base_model = MobileNet(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape\n",
        "            )\n",
        "            x = base_model(inputs)\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        elif self.version == 'v2':\n",
        "            base_model = MobileNetV2(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape\n",
        "            )\n",
        "            x = base_model(inputs)\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        elif self.version == 'v3-small':\n",
        "            base_model = MobileNetV3Small(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape,\n",
        "                pooling='avg'\n",
        "            )\n",
        "            x = base_model(inputs)\n",
        "\n",
        "        elif self.version == 'v3-large':\n",
        "            base_model = MobileNetV3Large(\n",
        "                weights='imagenet',\n",
        "                include_top=False,\n",
        "                input_shape=self.input_shape,\n",
        "                pooling='avg'\n",
        "            )\n",
        "            x = base_model(inputs)\n",
        "\n",
        "        base_model.trainable = False\n",
        "        outputs = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        model_path = os.path.join(save_path, f'mobilenet_{self.version}_model.h5')\n",
        "        self.model.save(model_path)\n",
        "\n",
        "        metadata = {\n",
        "            'version': self.version,\n",
        "            'best_val_accuracy': self.best_val_accuracy,\n",
        "            'training_time': self.training_time,\n",
        "            'history': self.history.history if self.history else None\n",
        "        }\n",
        "\n",
        "        metadata_path = os.path.join(save_path, f'mobilenet_{self.version}_metadata.json')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f)\n",
        "\n",
        "        print(f\"모델이 저장되었습니다: {model_path}\")\n",
        "        print(f\"메타데이터가 저장되었습니다: {metadata_path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, load_path, version):\n",
        "        instance = cls(version)\n",
        "        model_path = os.path.join(load_path, f'mobilenet_{version}_model.h5')\n",
        "        metadata_path = os.path.join(load_path, f'mobilenet_{version}_metadata.json')\n",
        "\n",
        "        if os.path.exists(model_path):\n",
        "            instance.model = tf.keras.models.load_model(model_path)\n",
        "            print(f\"모델을 불러왔습니다: {model_path}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"모델 파일을 찾을 수 없습니다: {model_path}\")\n",
        "\n",
        "        if os.path.exists(metadata_path):\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "\n",
        "            instance.best_val_accuracy = metadata['best_val_accuracy']\n",
        "            instance.training_time = metadata['training_time']\n",
        "            if metadata['history']:\n",
        "                instance.history = type('History', (), {'history': metadata['history']})\n",
        "\n",
        "            print(f\"메타데이터를 불러왔습니다: {metadata_path}\")\n",
        "\n",
        "        return instance\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=10):\n",
        "        print(f\"\\n=== MobileNet {self.version} 학습 시작 ===\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            min_delta=0.001,\n",
        "            patience=6,\n",
        "            mode='max',\n",
        "            verbose=1,\n",
        "            restore_best_weights=False\n",
        "        )\n",
        "\n",
        "        performance_threshold = PerformanceThresholdCallback(\n",
        "            accuracy_threshold=1,  # 100% 정확도\n",
        "            loss_threshold=0.05    # 0.05 이하 손실\n",
        "        )\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=32,\n",
        "            callbacks=[early_stop, performance_threshold],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        if performance_threshold.best_weights is not None:\n",
        "            print(\"\\n임계값을 넘는 best weights가 발견되어 복원합니다.\")\n",
        "            self.model.set_weights(performance_threshold.best_weights)\n",
        "            self.best_val_accuracy = performance_threshold.best_accuracy\n",
        "        else:\n",
        "            print(\"\\n임계값을 넘는 모델이 없어 현재 weights를 유지합니다.\")\n",
        "            self.best_val_accuracy = max(self.history.history['val_accuracy'])\n",
        "\n",
        "        self.training_time = time.time() - start_time\n",
        "        print(f\"학습 시간: {self.training_time:.2f}초\")\n",
        "\n",
        "    def evaluate(self, X_test, y_test, class_names):\n",
        "        inference_start_time = time.time()\n",
        "\n",
        "        # 예측 수행\n",
        "        y_pred_proba = self.model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "        inference_time = time.time() - inference_start_time\n",
        "\n",
        "        # 종합적인 성능 평가\n",
        "        evaluation_results = evaluate_metrics(y_test, y_pred, y_pred_proba, class_names)\n",
        "\n",
        "        # ROC 커브 그리기\n",
        "        plot_roc_curves(y_test, y_pred_proba, class_names)\n",
        "\n",
        "        # MAP 계산\n",
        "        ap_scores, map_score = calculate_map(y_test, y_pred_proba, class_names)\n",
        "\n",
        "        # 결과 출력\n",
        "        print(\"\\n=== 모델 성능 평가 결과 ===\")\n",
        "        print(f\"\\n1. 기본 메트릭:\")\n",
        "        for metric, value in evaluation_results['basic_metrics'].items():\n",
        "            if value is not None:\n",
        "                print(f\"- {metric}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\n2. 클래스별 성능:\")\n",
        "        for class_name in class_names:\n",
        "            metrics = evaluation_results['class_report'][class_name]\n",
        "            print(f\"\\n{class_name}:\")\n",
        "            print(f\"- Precision: {metrics['precision']:.4f}\")\n",
        "            print(f\"- Recall: {metrics['recall']:.4f}\")\n",
        "            print(f\"- F1-score: {metrics['f1-score']:.4f}\")\n",
        "            print(f\"- Support: {metrics['support']}\")\n",
        "\n",
        "        print(f\"\\n3. 처리 시간:\")\n",
        "        print(f\"- 전체 추론 시간: {inference_time:.2f}초\")\n",
        "        print(f\"- 이미지당 평균 추론 시간: {(inference_time/len(X_test))*1000:.2f}ms\")\n",
        "\n",
        "        self._plot_training_history()\n",
        "        self._plot_confusion_matrix(evaluation_results['confusion_matrix'], class_names)\n",
        "\n",
        "        return {\n",
        "            'metrics': evaluation_results['basic_metrics'],\n",
        "            'class_report': evaluation_results['class_report'],\n",
        "            'confusion_matrix': evaluation_results['confusion_matrix'],\n",
        "            'class_accuracy': evaluation_results['class_accuracy'],\n",
        "            'training_time': self.training_time,\n",
        "            'inference_time': inference_time,\n",
        "            'inference_time_per_image': inference_time/len(X_test),\n",
        "            'best_val_accuracy': self.best_val_accuracy,\n",
        "            'best_val_loss': min(self.history.history['val_loss']),\n",
        "            'ap_scores': ap_scores,\n",
        "            'map_score': map_score\n",
        "        }\n",
        "\n",
        "    def _plot_training_history(self):\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history.history['accuracy'], label='train')\n",
        "        plt.plot(self.history.history['val_accuracy'], label='validation')\n",
        "        plt.title(f'MobileNet {self.version} Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.history.history['loss'], label='train')\n",
        "        plt.plot(self.history.history['val_loss'], label='validation')\n",
        "        plt.title(f'MobileNet {self.version} Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm, class_names):\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(\n",
        "            cm,\n",
        "            annot=True,\n",
        "            fmt='d',\n",
        "            cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names\n",
        "        )\n",
        "        plt.title(f'MobileNet {self.version} Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. 경로 설정\n",
        "    base_path = '/content/drive/MyDrive/augmented_data'\n",
        "    test_path = '/content/drive/MyDrive/test_data2'\n",
        "    model_save_path = '/content/drive/MyDrive/saved_models'  # 모델 저장 경로\n",
        "\n",
        "    # 2. 데이터 로드\n",
        "    print(\"=== 데이터 로드 시작 ===\")\n",
        "    train_images, train_labels = load_data(base_path, num_samples=102)\n",
        "    test_images, test_labels = load_test_data(test_path)\n",
        "\n",
        "    # 데이터 타입 확인 및 변환\n",
        "    train_images = train_images.astype('float32')\n",
        "    test_images = test_images.astype('float32')\n",
        "\n",
        "    # 3. 학습/검증 데이터 분할\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        train_images,\n",
        "        train_labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=train_labels\n",
        "    )\n",
        "\n",
        "    # 4. 모델 생성 및 학습\n",
        "    version = 'v2'\n",
        "    experiment = MobileNetExperiment(version)\n",
        "\n",
        "    try:\n",
        "        # 5. 저장된 모델 불러오기\n",
        "        loaded_experiment = MobileNetExperiment.load_model(model_save_path, version)\n",
        "        print(\"저장된 모델을 성공적으로 불러왔습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"모델 로드 중 에러 발생: {str(e)}\")\n",
        "        print(\"새로운 모델을 학습합니다.\")\n",
        "        experiment.train(X_train, y_train, X_val, y_val)\n",
        "        experiment.save_model(model_save_path)\n",
        "        loaded_experiment = experiment\n",
        "\n",
        "    # 6. 모델 평가\n",
        "    class_names = ['CN7', 'G2', 'BLDC-400-f', 'BLDC-400-b']\n",
        "    results = loaded_experiment.evaluate(test_images, test_labels, class_names)"
      ],
      "metadata": {
        "id": "pt6-jLbSn3DU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}